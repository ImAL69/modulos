<!DOCTYPE html>
<html lang="es">
<head>
<meta charset="UTF-8">
<title>Pruebas adversarias, Ética y Efectividad</title>
<style>
    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;800&family=Open+Sans:wght@400;600&display=swap');

    body { background:#f5f5f5; margin:0; padding:0; }

    .unit-container {
        font-family:'Open Sans',sans-serif;
        color:#444;
        max-width:1100px;
        margin:40px auto;
        padding:60px 30px;
        background:#fff;
        border-radius:20px;
        box-shadow:0 10px 40px rgba(0,0,0,0.08);
        position:relative;
        border-top:8px solid #0277bd;
    }

    .uni-logo{
        position:absolute; top:30px; right:30px; width:120px;
        background:rgba(255,255,255,0.9); padding:5px;
        border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);
    }

    .header-box{ text-align:center; margin-bottom:30px; }
    .unit-title{
        font-family:'Montserrat',sans-serif; font-size:2.2rem;
        font-weight:800; text-transform:uppercase; color:#01579b;
        margin:0 0 8px 0;
    }
    .unit-subtitle{
        font-size:1rem; color:#0288d1; font-weight:600; margin-bottom:20px;
    }

    .hero-img{
        width:100%; max-width:820px; border-radius:15px;
        margin:0 auto 25px auto; display:block;
        box-shadow:0 20px 40px rgba(1,87,155,0.15);
    }

    .section{ margin-bottom:32px; font-size:0.98rem; line-height:1.7; text-align:justify; }
    .section h2{
        font-family:'Montserrat',sans-serif; color:#01579b; font-size:1.5rem;
        margin-bottom:10px;
    }
    .section h3{
        font-family:'Montserrat',sans-serif; color:#0277bd; font-size:1.1rem;
        margin-bottom:6px; margin-top:14px;
    }

    .highlight-box{
        background:#e1f5fe; border-left:5px solid #0288d1;
        border-radius:10px; padding:14px 16px; margin-bottom:20px;
    }

    ul{ padding-left:22px; margin-top:6px; }
    li{ margin-bottom:6px; }

    .resources-box{
        border-radius:14px; padding:18px 20px; margin-bottom:10px;
        background:#f1f8e9; border-left:5px solid #7cb342;
    }

    @media (max-width:768px){
        .unit-container{ padding:30px 15px; margin:20px 10px; }
        .uni-logo{ position:static; margin:0 auto 15px auto; display:block; }
    }
</style>
</head>
<body>

<div class="unit-container">

<img class="uni-logo" src="https://www.dropbox.com/scl/fi/j7mrxq795h9ekhifpcink/logo.png?rlkey=bsh9h5f98pd67n3bfejh368vk&raw=1" alt="Logo institucional">

<div class="header-box">
    <span class="unit-subtitle">Unidad 2 – Fundamentos de IA y Ciberseguridad</span>
    <h1 class="unit-title">Pruebas adversarias, Ética y Efectividad</h1>
    <img class="hero-img"
         src="https://www.dropbox.com/scl/fi/2xz6nue23wwf925qroqko/Gemini_Generated_Image_mr074zmr074zmr07-1.png?rlkey=5hqxval8rg0vapgjrvnotd5dl&raw=1"
         alt="Pruebas adversarias y ética en IA">
</div>

<div class="section">
    <h2>Pruebas adversarias (Adversarial Testing)</h2>
    <div class="highlight-box">
        <p>
            Las <strong>pruebas adversarias</strong> se refieren a técnicas para evaluar la robustez y seguridad de sistemas,
            especialmente modelos de IA y <em>machine learning</em>, frente a ataques diseñados para engañarlos o explotarlos.
        </p>
    </div>

    <h3>Tipos de ataques adversarios</h3>
    <ul>
        <li><strong>Ejemplos adversarios (<em>adversarial examples</em>):</strong> entradas manipuladas para confundir modelos.</li>
        <li><strong>Ataques de evasión:</strong> modificación de <em>inputs</em> para evitar la detección.</li>
        <li><strong>Ataques de envenenamiento (<em>poisoning</em>):</strong> contaminación de los datos de entrenamiento.</li>
    </ul>

    <h3>Objetivo</h3>
    <ul>
        <li>Identificar vulnerabilidades técnicas de los modelos y sistemas.</li>
        <li>Mejorar la resiliencia y robustez frente a ataques reales.</li>
        <li>Definir límites de uso seguro y condiciones de operación.</li>
    </ul>

    <h3>Herramientas y frameworks</h3>
    <ul>
        <li>CleverHans.</li>
        <li>Foolbox.</li>
        <li>IBM Adversarial Robustness Toolbox.</li>
    </ul>
</div>

<div class="section">
    <h2>Ética en IA y ciberseguridad</h2>
    <div class="highlight-box">
        <p>
            La <strong>ética</strong> aborda el uso responsable, justo y transparente de tecnologías, especialmente IA,
            para evitar daños, discriminación, violaciones de privacidad y abuso.
        </p>
    </div>

    <h3>Principios clave</h3>
    <ul>
        <li>Justicia y no discriminación.</li>
        <li>Transparencia y explicabilidad.</li>
        <li>Responsabilidad y rendición de cuentas.</li>
        <li>Privacidad y seguridad.</li>
        <li>Consentimiento informado y respeto por la autonomía de las personas.</li>
    </ul>

    <h3>Desafíos éticos</h3>
    <ul>
        <li>Sesgos en datos y modelos que generan decisiones injustas.</li>
        <li>Decisiones automatizadas con poco o nulo control humano.</li>
        <li>Vigilancia masiva y erosión de la privacidad.</li>
        <li>Manipulación de información y comportamiento (desinformación, microtargeting).</li>
    </ul>

    <h3>Regulaciones y marcos</h3>
    <ul>
        <li><strong>GDPR / RGPD:</strong> protección de datos y derechos de los sujetos.</li>
        <li><strong>AI Act (UE):</strong> propuesta regulatoria para sistemas de IA según nivel de riesgo.</li>
        <li><strong>NIST AI Risk Management Framework:</strong> gestión del riesgo en sistemas de IA.</li>
    </ul>
</div>

<div class="section">
    <h2>Efectividad</h2>
    <div class="highlight-box">
        <p>
            La <strong>efectividad</strong> de las pruebas adversarias y de los controles éticos se mide por su capacidad
            para detectar y mitigar riesgos reales, mejorar la confianza en los sistemas y apoyar el cumplimiento normativo.
        </p>
    </div>

    <h3>Medición de efectividad</h3>
    <ul>
        <li>Tasa de detección de ataques y vulnerabilidades.</li>
        <li>Reducción de falsos positivos y falsos negativos.</li>
        <li>Impacto en el desempeño del sistema (latencia, precisión, disponibilidad).</li>
        <li>Nivel de aceptación social y confianza de usuarios y stakeholders.</li>
    </ul>

    <h3>Balance práctico</h3>
    <ul>
        <li>Equilibrar seguridad y robustez con privacidad y usabilidad.</li>
        <li>Evitar controles tan restrictivos que bloqueen la innovación o el uso legítimo.</li>
        <li>Asegurar que las mitigaciones sean proporcionales al riesgo.</li>
    </ul>
</div>

<div class="section">
    <h2>Fuentes y referencias</h2>
    <div class="resources-box">
        <ul>
            <li><strong>Goodfellow, I., Shlens, J., &amp; Szegedy, C. (2015).</strong> Explaining and Harnessing Adversarial Examples (arXiv:1412.6572).</li>
            <li><strong>IBM Adversarial Robustness Toolbox:</strong> toolkit para pruebas de robustez adversaria.</li>
            <li><strong>European Commission AI Act proposal:</strong> aproximación europea a la regulación de la IA.</li>
            <li><strong>NIST AI Risk Management Framework:</strong> marco para gestionar riesgos en IA.</li>
            <li><strong>IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems:</strong> lineamientos éticos para sistemas autónomos e inteligentes.</li>
        </ul>
    </div>
</div>

</div>

</body>
</html>
